# Builds ONNX Runtime from source for optimized CPU inference.
# Source builds produce faster code than generic PyPI wheels (~1.7x on Strix Halo).
# First build will be slow (~30–60 min); subsequent builds are fast via Docker layer cache.

# ── Stage 1: compile ONNX Runtime ────────────────────────────────────────────
FROM ubuntu:22.04 AS ort-builder

ARG ORT_REF=v1.24.2
ARG DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential git wget \
        python3 python3.10-dev python3-pip \
        libprotobuf-dev protobuf-compiler \
    && rm -rf /var/lib/apt/lists/*

# Use pip cmake/ninja — apt cmake on 22.04 is 3.22, ORT needs 3.26+
RUN pip3 install --no-cache-dir "cmake<4" ninja numpy packaging

# Eigen is pre-cloned by the workflow on the GHA runner (which has full network access)
# and passed in via build context as eigen-src/. BuildKit containers can't reach GitLab
# or github.com/archive (codeload.github.com) where ORT's FetchContent would go.
COPY eigen-src /tmp/eigen-src

WORKDIR /build

RUN git clone --depth 1 --branch ${ORT_REF} \
    https://github.com/microsoft/onnxruntime.git

RUN cd onnxruntime && ./build.sh \
        --allow_running_as_root \
        --config Release \
        --build_shared_lib \
        --parallel \
        --enable_pybind \
        --build_wheel \
        --skip_tests \
        --skip_submodule_sync \
        --cmake_extra_defines \
            onnxruntime_BUILD_UNIT_TESTS=OFF \
            FETCHCONTENT_SOURCE_DIR_EIGEN=/tmp/eigen-src

# ── Stage 2: lean runtime image ──────────────────────────────────────────────
# Must match the Python version used in stage 1 (Ubuntu 22.04 default = 3.10)
FROM python:3.10-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install source-built ORT wheel, then clear executable stack flag.
# ORT MLAS assembly marks PT_GNU_STACK as RWE; kernels 6.18+ refuse dlopen.
COPY --from=ort-builder /build/onnxruntime/build/Linux/Release/dist/*.whl /tmp/
COPY fix_execstack.py /tmp/fix_execstack.py
RUN pip install --no-cache-dir /tmp/onnxruntime*.whl && rm /tmp/onnxruntime*.whl \
    && python3 /tmp/fix_execstack.py '/usr/local/lib/python3.10/**/onnxruntime/**/*.so' \
    && rm /tmp/fix_execstack.py

# Install kittentts without deps to avoid pulling in plain onnxruntime over our source build
RUN pip install --no-cache-dir --no-deps \
    "kittentts @ https://github.com/KittenML/KittenTTS/releases/download/0.8/kittentts-0.8.0-py3-none-any.whl" \
    && pip install --no-cache-dir \
    espeakng_loader huggingface_hub misaki phonemizer-fork num2words numpy soundfile spacy \
    fastapi \
    "uvicorn[standard]"

COPY server.py .

ENV KITTENTTS_MODEL=KittenML/kitten-tts-mini-0.8
ENV KITTENTTS_PORT=8080

EXPOSE 8080

CMD ["python", "server.py"]
