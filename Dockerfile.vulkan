# Builds ONNX Runtime from source with the experimental Vulkan execution provider.
# The Vulkan EP was added in ONNX Runtime 1.21+ and is not included in PyPI packages.
# First build will be slow (~30–60 min); subsequent builds are fast via Docker layer cache.

# ── Stage 1: compile ONNX Runtime with Vulkan EP ─────────────────────────────
FROM ubuntu:22.04 AS ort-builder

ARG ORT_REF=v1.21.0
ARG DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential git wget \
        python3 python3.10-dev python3-pip \
        libvulkan-dev glslang-tools \
        libprotobuf-dev protobuf-compiler \
    && rm -rf /var/lib/apt/lists/*

# Use pip cmake/ninja — apt cmake on 22.04 is 3.22, ORT needs 3.26+
RUN pip3 install --no-cache-dir "cmake<4" ninja numpy packaging

# Eigen is pre-cloned by the workflow on the GHA runner (which has full network access)
# and passed in via build context as eigen-src/. BuildKit containers can't reach GitLab
# or github.com/archive (codeload.github.com) where ORT's FetchContent would go.
COPY eigen-src /tmp/eigen-src

WORKDIR /build

RUN git clone --depth 1 --branch ${ORT_REF} \
    https://github.com/microsoft/onnxruntime.git

RUN cd onnxruntime && ./build.sh \
        --allow_running_as_root \
        --config Release \
        --build_shared_lib \
        --parallel \
        --enable_pybind \
        --build_wheel \
        --skip_tests \
        --skip_submodule_sync \
        --cmake_extra_defines \
            onnxruntime_USE_VULKAN=ON \
            onnxruntime_BUILD_UNIT_TESTS=OFF \
            FETCHCONTENT_SOURCE_DIR_EIGEN=/tmp/eigen-src

# ── Stage 2: lean runtime image ──────────────────────────────────────────────
# Must match the Python version used in stage 1 (Ubuntu 22.04 default = 3.10)
FROM python:3.10-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libvulkan1 \
        mesa-vulkan-drivers \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install the Vulkan-enabled ORT wheel, then clear executable stack flag
# from .so files. ORT MLAS assembly marks PT_GNU_STACK as RWE (executable);
# newer kernels (6.18+) refuse dlopen with EINVAL when this is set.
# Patch the ELF program header to change RWE→RW (flags 0x7→0x6).
COPY --from=ort-builder /build/onnxruntime/build/Linux/Release/dist/*.whl /tmp/
RUN pip install --no-cache-dir /tmp/onnxruntime*.whl && rm /tmp/onnxruntime*.whl \
    && python3 -c " \
import glob, struct, os; \
PT_GNU_STACK = 0x6474e551; \
PF_X = 0x1; \
for so in glob.glob('/usr/local/lib/python3.10/**/onnxruntime/**/*.so', recursive=True): \
    with open(so, 'r+b') as f: \
        magic = f.read(4); \
        if magic != b'\x7fELF': continue; \
        ei_class = f.read(1)[0]; \
        if ei_class != 2: continue; \
        f.seek(32); e_phoff = struct.unpack('<Q', f.read(8))[0]; \
        f.seek(54); e_phentsize = struct.unpack('<H', f.read(2))[0]; \
        e_phnum = struct.unpack('<H', f.read(2))[0]; \
        for i in range(e_phnum): \
            off = e_phoff + i * e_phentsize; \
            f.seek(off); p_type = struct.unpack('<I', f.read(4))[0]; \
            if p_type != PT_GNU_STACK: continue; \
            p_flags = struct.unpack('<I', f.read(4))[0]; \
            if p_flags & PF_X: \
                f.seek(off + 4); f.write(struct.pack('<I', p_flags & ~PF_X)); \
                print(f'Cleared execstack: {so}'); \
            break \
"

# Install kittentts without deps to avoid pulling in plain onnxruntime over our Vulkan build
RUN pip install --no-cache-dir --no-deps \
    "kittentts @ https://github.com/KittenML/KittenTTS/releases/download/0.8/kittentts-0.8.0-py3-none-any.whl" \
    && pip install --no-cache-dir \
    espeakng_loader huggingface_hub misaki num2words numpy soundfile spacy \
    fastapi \
    "uvicorn[standard]"

COPY server.py .

ENV KITTENTTS_MODEL=KittenML/kitten-tts-mini-0.8
ENV KITTENTTS_PORT=8080

EXPOSE 8080

CMD ["python", "server.py"]
